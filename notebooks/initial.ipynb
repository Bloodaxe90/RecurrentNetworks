{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T12:47:41.042970Z",
     "start_time": "2025-04-06T12:47:30.968754Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/v8/_7nzg4ln01d8js37_knftkp40000gp/T/ipykernel_5258/654635744.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/Eric/PycharmProjects/RecurrentNetworks/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T19:38:27.974882Z",
     "start_time": "2025-04-06T19:38:27.953576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RecurrentLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedded_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.hidden_w: torch.Tensor = nn.Parameter(torch.ones(hidden_dim, hidden_dim))\n",
    "        self.input_w: torch.Tensor = nn.Parameter(torch.ones(embedded_dim, hidden_dim))\n",
    "        self.b: torch.Tensor = nn.Parameter(torch.ones(hidden_dim))\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x_seq: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, input_dim = x.shape[:-1]\n",
    "        hidden_state = torch.zeros(batch_size, self.hidden_dim).to(x_seq.device.type)\n",
    "\n",
    "        hidden_states = torch.Tensor()\n",
    "        for step in range(input_dim):\n",
    "            z = torch.matmul(hidden_state, self.hidden_w) + torch.matmul(x_seq[..., step, :], self.input_w) + self.b\n",
    "            hidden_state = self.tanh(z)\n",
    "            hidden_states = torch.cat((hidden_states, hidden_state.unsqueeze(1)), dim= -2)\n",
    "        return hidden_states\n",
    "\n",
    "x = torch.randn((4, 2, 3))\n",
    "rnn = RecurrentLayer(3, 6)\n",
    "logit = rnn(x)\n",
    "logit.shape"
   ],
   "id": "baea2647bdcb57fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 12])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T19:39:58.679888Z",
     "start_time": "2025-04-06T19:39:58.630243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Gate(nn.Module):\n",
    "\n",
    "    def __init__(self, embedded_dim: int, hidden_dim: int, activation_function: nn.Module):\n",
    "        super().__init__()\n",
    "        self.hidden_w = nn.Parameter(torch.ones((hidden_dim, hidden_dim)))\n",
    "        self.input_w = nn.Parameter(torch.ones((embedded_dim, hidden_dim)))\n",
    "        self.b = nn.Parameter(torch.ones(hidden_dim))\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        z = (torch.matmul(h, self.hidden_w) + torch.matmul(x, self.input_w)) + self.b\n",
    "        return self.activation_function(z)\n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedded_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.forget_gate = Gate(embedded_dim, hidden_dim, nn.Sigmoid())\n",
    "        self.input_gate = Gate(embedded_dim, hidden_dim, nn.Sigmoid())\n",
    "        self.candidate_layer = Gate(embedded_dim, hidden_dim, nn.Tanh())\n",
    "        self.output_gate = Gate(embedded_dim, hidden_dim, nn.Sigmoid())\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, input_dim = x.shape[:-1]\n",
    "        device = x.device.type\n",
    "        c_t = torch.zeros((batch_size, self.hidden_dim)).to(device)\n",
    "        h_t = torch.zeros((batch_size, self.hidden_dim)).to(device)\n",
    "\n",
    "        hidden_states: torch.Tensor = torch.Tensor()\n",
    "        for step in range(input_dim):\n",
    "            x_t = x[:, step, :]\n",
    "            f_t = self.forget_gate(x_t, h_t)\n",
    "            i_t = self.input_gate(x_t, h_t)\n",
    "            candidate_c = self.candidate_layer(x_t, h_t)\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * candidate_c)\n",
    "            o_t = self.output_gate(x_t, h_t)\n",
    "            h_t = o_t * self.tanh(c_t)\n",
    "            hidden_states = torch.cat((hidden_states, h_t.unsqueeze(1)), dim= -2)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "x = torch.randn((4, 2, 3))\n",
    "lstm = LSTMLayer(3, 6)\n",
    "logit = lstm(x)\n",
    "logit.shape"
   ],
   "id": "5780be8eccde534d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 6])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T21:00:02.027295Z",
     "start_time": "2025-04-06T21:00:01.962786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GRULayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedded_dim: int, hidden_dim: int, reset_first: bool = False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.reset_first = reset_first\n",
    "\n",
    "        self.reset_gate = Gate(embedded_dim, hidden_dim, nn.Sigmoid())\n",
    "        self.update_gate = Gate(embedded_dim, hidden_dim, nn.Sigmoid())\n",
    "\n",
    "        self.hidden_w = nn.Parameter(torch.ones((hidden_dim, hidden_dim)))\n",
    "        self.input_w = nn.Parameter(torch.ones((embedded_dim, hidden_dim)))\n",
    "        self.bias = nn.Parameter(torch.ones(hidden_dim))\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, input_dim = x.shape[:-1]\n",
    "        h_t = torch.zeros((batch_size, self.hidden_dim)).to(x.device.type)\n",
    "\n",
    "        hidden_states: torch.Tensor = torch.Tensor()\n",
    "        for step in range(input_dim):\n",
    "            x_t = x[:, step, :]\n",
    "            r_t = self.reset_gate(x_t, h_t)\n",
    "            z_t = self.update_gate(x_t, h_t)\n",
    "\n",
    "            candidate_h = self.tanh((r_t * torch.matmul(h_t, self.hidden_w)) + torch.matmul(x_t, self.input_w) + self.bias)\n",
    "\n",
    "            h_t = (z_t * h_t) + ((1 - z_t) * candidate_h)\n",
    "            hidden_states = torch.cat((hidden_states, h_t.unsqueeze(1)), dim= -2)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "x = torch.randn((4, 2, 3))\n",
    "gru = GRULayer(3, 6)\n",
    "logit = gru(x)\n",
    "logit.shape\n",
    "\n",
    "\n"
   ],
   "id": "1303463c88dae6e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
